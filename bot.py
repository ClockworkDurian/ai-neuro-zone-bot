# bot.py ‚Äî –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π (aiogram 3.x). –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —Å llm_core.py (–Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è).
import asyncio
import logging
import os
import time
from collections import defaultdict, deque

from aiogram import Bot, Dispatcher, types
from aiogram.client.default import DefaultBotProperties
from aiogram.filters import Command
from aiogram.exceptions import TelegramAPIError
from aiogram.types import InlineKeyboardButton, InlineKeyboardMarkup

from llm_core import (
    generate_text_stream,
    generate_text,
    generate_image,
    trim_history_by_tokens
)

# -----------------------
# logging
# -----------------------
logger = logging.getLogger("neurozone_bot")
logger.setLevel(logging.INFO)
h = logging.StreamHandler()
h.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
logger.addHandler(h)

def safe_log(uid, event, extra=None):
    d = {"user_id": uid, "event": event}
    if extra:
        d.update(extra)
    logger.info(d)

# -----------------------
# env
# -----------------------
BOT_TOKEN = os.getenv("BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GROK_API_KEY = os.getenv("GROK_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not BOT_TOKEN:
    raise SystemExit("BOT_TOKEN not set")

bot = Bot(token=BOT_TOKEN, default=DefaultBotProperties(parse_mode="HTML"))
dp = Dispatcher()

# -----------------------
# rate limit
# -----------------------
RATE_LIMIT_PER_MINUTE = 30
user_requests = defaultdict(lambda: deque())

def check_rate_limit(uid: int) -> bool:
    now = time.time()
    dq = user_requests[uid]
    while dq and dq[0] < now - 60:
        dq.popleft()
    if len(dq) >= RATE_LIMIT_PER_MINUTE:
        return False
    dq.append(now)
    return True

# -----------------------
# user state
# -----------------------
MAX_HISTORY_TOKENS_DEFAULT = 3000
user_state = defaultdict(lambda: {
    "mode": None,
    "provider": None,
    "model": None,
    "history": [],
    "max_history_tokens": MAX_HISTORY_TOKENS_DEFAULT
})

# temporarily unavailable providers map (provider -> unix_ts_available)
provider_unavailable = {}

# -----------------------
# models (text + image)
# -----------------------
openai_models = {
    "GPT-5": {"id": "gpt-5", "desc": "–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞ –∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö –∑–∞–¥–∞—á."},
    "GPT-5 mini": {"id": "gpt-5-mini", "desc": "–ë–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è, —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è."},
    "GPT-5 nano": {"id": "gpt-5-nano", "desc": "–°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è."},
    "GPT-4.1": {"id": "gpt-4.1", "desc": "–°–∞–º–∞—è —É–º–Ω–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."}
}
grok_models = {
    "Grok-code-fast-1": {"id": "grok-code-fast-1", "desc": "–ë—ã—Å—Ç—Ä–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è."},
    "Grok-4-fast-reasoning": {"id": "grok-4-fast-reasoning", "desc": "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –≤ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö."},
    "Grok-4-fast-non-reasoning": {"id": "grok-4-fast-non-reasoning", "desc": "–ü–æ—Å–ª–µ–¥–Ω–µ–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –≤ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö."}
}
gemini_models = {
    "Gemini 2.5 Flash": {"id": "gemini-2.5-flash", "desc": "–õ—É—á—à–∞—è –ø–æ —Ü–µ–Ω–µ/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."},
    "Gemini 2.5 Flash-Lite": {"id": "gemini-2.5-flash-lite", "desc": "–°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è flash-–º–æ–¥–µ–ª—å."}
}

# image models
openai_image_models = {
    "DALL¬∑E 3": {"id": "dall-e-3", "desc": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (OpenAI)."}
}
grok_image_models = {
    "Grok Vision": {"id": "grok-image-1", "desc": "Grok image generation."}
}
gemini_image_models = {
    "Gemini Image": {"id": "gemini-image-1", "desc": "Gemini image model (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)."}
}

# -----------------------
# keyboards
# -----------------------
def kb_main():
    return InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="–¢–µ–∫—Å—Ç", callback_data="mode:text"),
         InlineKeyboardButton(text="–ö–∞—Ä—Ç–∏–Ω–∫–∏", callback_data="mode:image")],
        [InlineKeyboardButton(text="OpenAI", callback_data="provider:openai"),
         InlineKeyboardButton(text="Grok", callback_data="provider:grok"),
         InlineKeyboardButton(text="Gemini", callback_data="provider:gemini")],
        [InlineKeyboardButton(text="–°–±—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏", callback_data="reset:history")]
    ])

def model_selected_menu():
    return InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="‚¨ÖÔ∏è –ù–∞–∑–∞–¥ –∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º", callback_data="back:providers")],
        [InlineKeyboardButton(text="‚¨ÖÔ∏è –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é", callback_data="back:main")]
    ])

def menu_after_answer():
    return InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="‚¨ÖÔ∏è –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é", callback_data="back:main")]
    ])

async def repost_menu(chat_id: int):
    try:
        await bot.send_message(chat_id, "–ú–µ–Ω—é:", reply_markup=kb_main())
    except Exception:
        pass

# -----------------------
# show models (text vs image)
# -----------------------
async def show_models_for_provider(cb: types.CallbackQuery, provider_key: str):
    uid = cb.from_user.id
    mode = user_state[uid].get("mode", "text")
    if mode == "image":
        if provider_key == "openai":
            models_dict = openai_image_models; header = "üñºÔ∏è <b>OpenAI ‚Äî Image</b>"
        elif provider_key == "grok":
            models_dict = grok_image_models; header = "üñºÔ∏è <b>Grok ‚Äî Image</b>"
        else:
            models_dict = gemini_image_models; header = "üñºÔ∏è <b>Gemini ‚Äî Image</b>"
    else:
        if provider_key == "openai":
            models_dict = openai_models; header = "üîµ <b>OpenAI ‚Äî ChatGPT</b>"
        elif provider_key == "grok":
            models_dict = grok_models; header = "üß† <b>Grok ‚Äî xAI</b>"
        else:
            models_dict = gemini_models; header = "‚ö° <b>Gemini ‚Äî Google</b>"

    parts = [f"{header}\n\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:"]
    kb_rows = []
    for name, meta in models_dict.items():
        parts.append(f"\n<b>{name}</b> ‚Äî <i>{meta['desc']}</i>")
        kb_rows.append([InlineKeyboardButton(text=name, callback_data=f"model:{meta['id']}")])
    kb_rows.append([InlineKeyboardButton(text="‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="back:providers")])
    txt = "\n".join(parts)

    try:
        await cb.message.edit_text(txt, reply_markup=InlineKeyboardMarkup(inline_keyboard=kb_rows))
    except Exception:
        await cb.message.answer(txt, reply_markup=InlineKeyboardMarkup(inline_keyboard=kb_rows))
    await cb.answer()

# -----------------------
# callbacks
# -----------------------
@dp.message(Command("start", "help"))
async def cmd_start(message: types.Message):
    uid = message.from_user.id
    safe_log(uid, "start")
    await message.answer("–ü—Ä–∏–≤–µ—Ç! –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", reply_markup=kb_main())

@dp.callback_query(lambda c: c.data and c.data.startswith("mode:"))
async def cb_mode(cb: types.CallbackQuery):
    uid = cb.from_user.id
    mode = cb.data.split(":",1)[1]
    user_state[uid]["mode"] = mode
    safe_log(uid, "mode_set", {"mode": mode})
    await cb.message.edit_text(f"–†–µ–∂–∏–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: <b>{mode}</b>\n–¢–µ–ø–µ—Ä—å –≤—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞:", reply_markup=kb_main())
    await cb.answer()

@dp.callback_query(lambda c: c.data and c.data.startswith("provider:"))
async def cb_provider(cb: types.CallbackQuery):
    uid = cb.from_user.id
    prov = cb.data.split(":",1)[1]
    user_state[uid]["provider"] = prov
    safe_log(uid, "provider_set", {"provider": prov})
    await show_models_for_provider(cb, prov)

@dp.callback_query(lambda c: c.data == "back:providers")
async def cb_back_providers(cb: types.CallbackQuery):
    await cb.message.edit_text("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞:", reply_markup=kb_main())
    await cb.answer()

@dp.callback_query(lambda c: c.data and c.data.startswith("model:"))
async def cb_model(cb: types.CallbackQuery):
    uid = cb.from_user.id
    model = cb.data.split(":",1)[1]
    user_state[uid]["model"] = model
    safe_log(uid, "model_set", {"model": model})
    await cb.message.edit_text(f"–í—ã –≤—ã–±—Ä–∞–ª–∏ –º–æ–¥–µ–ª—å:\n<b>{model}</b>\n\n–¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å.", reply_markup=model_selected_menu())
    await cb.answer()

@dp.callback_query(lambda c: c.data == "reset:history")
async def cb_reset_history(cb: types.CallbackQuery):
    uid = cb.from_user.id
    user_state[uid]["history"] = []
    safe_log(uid, "history_reset")
    await cb.message.edit_text("–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞.", reply_markup=kb_main())
    await cb.answer()

@dp.callback_query(lambda c: c.data == "back:main")
async def cb_back_main(cb: types.CallbackQuery):
    await cb.message.edit_text("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é:", reply_markup=kb_main())
    await cb.answer()

# -----------------------
# message handler
# -----------------------
@dp.message()
async def on_message(message: types.Message):
    uid = message.from_user.id
    text = message.text or ""
    safe_log(uid, "msg_received", {"len": len(text)})

    # check rate limit
    if not check_rate_limit(uid):
        safe_log(uid, "rate_limited")
        await message.answer("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–¥–æ–∂–¥–∏—Ç–µ –º–∏–Ω—É—Ç—É.")
        return

    st = user_state[uid]
    mode = st.get("mode")
    provider = st.get("provider")
    model = st.get("model")

    if not mode:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —á–µ—Ä–µ–∑ /start")
        return
    if not provider:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.")
        return
    if not model:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å.")
        return

    # check provider availability (quota)
    now = time.time()
    if provider in provider_unavailable and provider_unavailable[provider] > now:
        await message.answer(f"–ü—Ä–æ–≤–∞–π–¥–µ—Ä {provider} –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (quota). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        return
    elif provider in provider_unavailable:
        del provider_unavailable[provider]

    # IMAGE mode
    if mode == "image":
        await message.answer("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
        try:
            url = await generate_image(provider=provider, prompt=text,
                                       openai_key=OPENAI_API_KEY, grok_key=GROK_API_KEY,
                                       gemini_key=GEMINI_API_KEY)
            await message.answer_photo(url, caption="–ì–æ—Ç–æ–≤–æ!")
        except Exception as e:
            safe_log(uid, "image_error", {"err": str(e)})
            # if gemini quota marker - mark provider
            if isinstance(e, RuntimeError) and str(e) == "GEMINI_QUOTA_EXCEEDED":
                provider_unavailable["gemini"] = time.time() + 10*60
                await message.answer("Gemini: quota exceeded. –ü—Ä–æ–≤–∞–π–¥–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á—ë–Ω –Ω–∞ 10 –º–∏–Ω—É—Ç.")
            else:
                await message.answer("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
        await repost_menu(message.chat.id)
        return

    # TEXT mode
    st["history"].append({"role":"user", "content": text})
    st["history"] = trim_history_by_tokens(st["history"], st.get("max_history_tokens", MAX_HISTORY_TOKENS_DEFAULT))

    status = await message.answer("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
    try:
        full = ""
        last_edit = time.time()
        stream = generate_text_stream(provider=provider, model=model,
                                      history=st["history"], user_input=text,
                                      openai_key=OPENAI_API_KEY, grok_key=GROK_API_KEY,
                                      gemini_key=GEMINI_API_KEY,
                                      max_history_tokens=st.get("max_history_tokens", MAX_HISTORY_TOKENS_DEFAULT))
        async for chunk in stream:
            full += chunk
            if time.time() - last_edit > 0.35:
                try:
                    await status.edit_text(full)
                except TelegramAPIError:
                    pass
                last_edit = time.time()

        try:
            await status.edit_text(full)
        except TelegramAPIError:
            pass

        st["history"].append({"role":"assistant", "content": full})
        st["history"] = trim_history_by_tokens(st["history"], st.get("max_history_tokens", MAX_HISTORY_TOKENS_DEFAULT))
    except Exception as e:
        safe_log(uid, "stream_error", {"err": str(e)})
        # handle gemini quota sentinel
        if isinstance(e, RuntimeError) and str(e) == "GEMINI_QUOTA_EXCEEDED":
            provider_unavailable["gemini"] = time.time() + 10*60
            try:
                await status.edit_text("Gemini: quota exceeded. –ü—Ä–æ–≤–∞–π–¥–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á—ë–Ω –Ω–∞ 10 –º–∏–Ω—É—Ç.")
            except Exception:
                pass
            await repost_menu(message.chat.id)
            return
        # fallback once
        try:
            fallback = await generate_text(provider=provider, model=model,
                                           history=st["history"], user_input=text,
                                           openai_key=OPENAI_API_KEY, grok_key=GROK_API_KEY,
                                           gemini_key=GEMINI_API_KEY,
                                           max_history_tokens=st.get("max_history_tokens", MAX_HISTORY_TOKENS_DEFAULT))
            await status.edit_text(fallback)
            st["history"].append({"role":"assistant", "content": fallback})
        except Exception as e2:
            safe_log(uid, "fallback_failed", {"err": str(e2)})
            try:
                await status.edit_text("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –º–æ–¥–µ–ª–∏.")
            except Exception:
                pass

    # repost menu so it stays at bottom
    await repost_menu(message.chat.id)

# -----------------------
# run
# -----------------------
async def main():
    logger.info("Bot started")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
